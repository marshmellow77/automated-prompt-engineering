{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/marshmellow77/automated-prompt-engineering/blob/main/automated-prompt-engineering.ipynb)\n",
    "\n",
    "\n",
    "This notebook demonstrates how to use Google's Gemini model to automate prompt engineering.\n",
    "\n",
    "Prompt engineering is a powerful way to improve the responses og large language models (LLMs). Bit it is also a manual, tedious, iterative process and it quickly accumulates technical debt and waste since each handcrafted prompt is specific to a model (and its version) as well as the task at hand.\n",
    "\n",
    "In this notebook we will learn how to use the DSPy library to autonomously and automatically create prompts that are optimised for a specific model and the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual prompt engineering is very tedious - let's look at an example where we carefully handcraft a prompt for our task and model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of 3 April 2024, VertexAI is not yet integrated into DSPy. But there already exists a PR for it which we can leverage.\n",
    "!pip install -U git+https://github.com/marshmellow77/dspy.git@seedstart-random-search#egg=dspy-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform\n",
    "!pip install Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not IS_COLAB:\n",
    "    raise ValueError(\"This notebook should be run using Google Colab.\")\n",
    "\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "project_id = \"cloud-llm-preview1\"\n",
    "vertexai.init(project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "gemini_pro = GenerativeModel(\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to use Gemini Pro for a mathematical text question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the fields `question`, produce the fields `answer`.\n",
    "\n",
    "Question: Heather is going to sew 150 aprons that are to be used for a kiddie crew program.\n",
    "She already was able to sew 13 aprons, and today, she sewed three times as many aprons.\n",
    "How many aprons should she sew tomorrow if she wants to sew half of the remaining number of aprons needed?\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# The correct answer is 49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"temperature\": 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini_pro.generate_content(contents=prompt, generation_config=config)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Gemini Pro got this one wrong. Let's use best practices including Chain of thought and few shot prompting to improve Gemini's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot prompting with Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Given the fields `question`, produce the fields `answer`.\n",
    "\n",
    "---\n",
    "\n",
    "Follow the following format.\n",
    "\n",
    "Question: <Question>\n",
    "Rationale: Let's think step by step ...\n",
    "Answer: <Answer>\n",
    "\n",
    "---\n",
    "\n",
    "Question: A gumball machine has red, green, and blue gumballs. The machine has half as many blue gumballs as red gumballs.\n",
    "For each blue gumball, the machine has 4 times as many green gumballs. If the machine has 16 red gumballs how many gumballs are in the machine?\n",
    "Rationale: Let's think step by step.\n",
    "First, we can find the number of blue gumballs in the machine.\n",
    "Since the machine has half as many blue gumballs as red gumballs, and there are 16 red gumballs, there must be 16 / 2 = 8 blue gumballs.\n",
    "Next, we can find the number of green gumballs in the machine.\n",
    "Since the machine has 4 times as many green gumballs as blue gumballs, there must be 8 x 4 = 32 green gumballs.\n",
    "Finally, we can add up the number of red, blue, and green gumballs to find the total number of gumballs in the machine: 16 + 8 + 32 = 56.\n",
    "Answer: 56\n",
    "\n",
    "---\n",
    "\n",
    "Question: Rachel makes $12.00 as a waitress in a coffee shop. In one hour, she serves 20 different people and they all leave her a $1.25 tip. How much money did she make in that hour?\n",
    "Rationale: Let's think step by step.\n",
    "First, we need to find out how much money Rachel made from tips. She served 20 people and each person left her a $1.25 tip, so she made 20 * $1.25 = $25.00 in tips.\n",
    "Next, we need to add her hourly wage to the money she made from tips to find out how much money she made in total. She made $12.00 per hour, so in one hour she made $12.00 + $25.00 = $37.00.\n",
    "Answer: 37\n",
    "\n",
    "---\n",
    "\n",
    "Question: Heather is going to sew 150 aprons that are to be used for a kiddie crew program. She already was able to sew 13 aprons, and today, she sewed three times as many aprons. How many aprons should she sew tomorrow if she wants to sew half of the remaining number of aprons needed?\n",
    "Rationale:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gemini_pro.generate_content(contents=prompt, generation_config=config)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, this worked!\n",
    "\n",
    "Now we have a good a good prompt for our model and the task at hand (mathematical text questions). But there are a few issues:\n",
    "* Our prompt works well on our model, but what if we want to use another model or another version (e.g. Gemini Ultra of Gemini 1.5)? Will it still work for those models?\n",
    "* We had to develop a few examples, and especially coming up with the rationale for each example was tedious\n",
    "\n",
    "The question is, could we automate this process so that next time we need to repeat this exercise we can just automatically create few shot examples that are optimised for our model and the task at hand?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated prompt engineering with DSPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy is a library that allows us to automate this process. Let's see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy_gemini_pro = dspy.GoogleVertexAI(\n",
    "    \"gemini-1.0-pro\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "dspy.settings.configure(lm=dspy_gemini_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [GSM8K dataset](https://paperswithcode.com/dataset/gsm8k) which consists of inguistically diverse grade school math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "\n",
    "gms8k = GSM8K()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = gms8k.train[:60], gms8k.dev[:20], gms8k.test[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].gold_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset has a field `gold_resoning`, which already provides reasoning. Since this is what we want to automate, let's delete these for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through datasets and modify the dicts\n",
    "for dataset in [train, val]:\n",
    "    for example in dataset:\n",
    "        example[\"gold_reasoning\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[0].gold_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signatures allow you tell the LM what it needs to do, rather than specify how we should ask the LM to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KSignature(dspy.Signature):\n",
    "    \"\"\"Answer math problems with numbers or short phrases.\"\"\"\n",
    "\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"Usually a number or short phrase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this signature to run a test with Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer = dspy.Predict(GSM8KSignature)\n",
    "pred = generate_answer(question=test[0].question)\n",
    "\n",
    "print(f\"Question: {test[0].question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Actual Answer: {test[0].answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy_gemini_pro.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above Gemini didn't get this one right. Let's evaluate Gemini of the test dataset to establish a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation with zero shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the evaluation programmatically we define a DSPy module These modules abstract a prompting technique (like chain of thought or ReAct). Crucially, they are generalized to handle any DSPy Signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM8KModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # here we use the dspy.Predict module which uses zero shot prompting to generate answers\n",
    "        self.prog = dspy.Predict(GSM8KSignature) \n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_zero_shot = GSM8KModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "NUM_THREADS = 4 # number of threads to use for parallel processing\n",
    "evaluate = Evaluate(\n",
    "    devset=test, # the test set\n",
    "    metric=gsm8k_metric, # the metric to use -> this will convert responses to integers to compare with the gold answers\n",
    "    num_threads=NUM_THREADS,\n",
    "    display_progress=True,\n",
    "    display_table=20, # how many rows to display\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gsm8k_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping few shot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will leverage Gemini Ultra to bootstrap few shot examples which will (hopefully) improve Gemini Pro's performance on the test dataset. With Gemini Ultra we will create a few reasoning examples which we can include in the prompt that we will eventually send to Gemini Pro. Ultra will produce a few candidates and test them on a validation dataset using the `gsm8k_metric`, i.e. the metric we want to optimise for. Once the best candidates have been identified these examples will then be used to create a few shot prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a Chain of Thought module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotCoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\n",
    "            GSM8KSignature,\n",
    "        )\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the bootstrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "RUN_FROM_SCRATCH = True\n",
    "bootstrapped_demos = 8 # how many examples are randomly being used from the training dataset\n",
    "labeled_demos = 3 # how many examples will be in final prompt\n",
    "candidate_programs = 2 # how many candidates will be created and evaluated (equivalent to epochs)\n",
    "teacher_model_id = \"gemini-1.0-ultra\" \n",
    "\n",
    "if RUN_FROM_SCRATCH:\n",
    "    dspy_gemini_ultra = dspy.GoogleVertexAI(\n",
    "        teacher_model_id,\n",
    "        temperature=0,\n",
    "    )\n",
    "    dspy.settings.configure(lm=dspy_gemini_ultra, timeout=0)\n",
    "    config = dict(\n",
    "        max_bootstrapped_demos=bootstrapped_demos,\n",
    "        max_labeled_demos=labeled_demos,\n",
    "        num_candidate_programs=candidate_programs,\n",
    "        num_threads=4,\n",
    "        stop_at_score=100.0,\n",
    "    )\n",
    "    bootstrap_optimizer = BootstrapFewShotWithRandomSearch(\n",
    "        metric=gsm8k_metric, **config\n",
    "    )\n",
    "    cot_fewshot = bootstrap_optimizer.compile(ZeroShotCoT(), trainset=train, valset=val, seed_start=0)\n",
    "\n",
    "    # save the bootstrap demonstrations for future use\n",
    "    timestamp_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = f\"{timestamp_str}_{teacher_model_id}_{bootstrapped_demos}_{labeled_demos}_{candidate_programs}.json\"\n",
    "    cot_fewshot.save(filename)\n",
    "else:\n",
    "    cot_fewshot = ZeroShotCoT()\n",
    "    cot_fewshot.load(\"20240403-173150_gemini-1.0-ultra_8_3_2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we have our examples ready, and we can test Gemini Pro on the same test dataset as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.settings.configure(lm=dspy_gemini_pro, timeout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(cot_fewshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, this improved Gemini Pro's performance significantly from 35% :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy_gemini_pro.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
